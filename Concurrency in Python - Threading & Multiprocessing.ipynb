{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "￼Discuss preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GIL\n",
    "￼\n",
    "On the Global Interpreter Lock Python 3.8.0 Alpha, [python/ceval_gil.h](https://github.com/python/cpython/blob/27e2d1f21975dfb8c0ddcb192fa0f45a51b7977e/Python/ceval_gil.h)\n",
    "\n",
    "Prevents multiple threads from the same process from executing CPython bytecode simultaneously on separate cores.\n",
    "\n",
    "Released under two conditions:\n",
    "1. Holding thread is performing I/O\n",
    "2. Another thread makes a gil_drop_request to the interpreter, which will (maybe) coordinate a GIL switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "def hello_worker():\n",
    "    for i in range(10):\n",
    "        print('Hello')\n",
    "\n",
    "def world_worker():\n",
    "    for i in range(10):\n",
    "        print('World')\n",
    "    \n",
    "threading.Thread(target=hello_worker).start()\n",
    "threading.Thread(target=world_worker).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context-switching every n milliseconds. Chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_worker():\n",
    "    for i in range(10):\n",
    "        time.sleep(0.01)        \n",
    "        print('Hello')\n",
    "\n",
    "def world_worker():\n",
    "    for i in range(10):\n",
    "        time.sleep(0.01)\n",
    "        print('World')\n",
    "    \n",
    "threading.Thread(target=hello_worker).start()\n",
    "threading.Thread(target=world_worker).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_worker(n):\n",
    "    print('Hello')\n",
    "    time.sleep(1)\n",
    "    print('Goodbye')\n",
    "    \n",
    "for i in range(4):\n",
    "        threading.Thread(target=hello_worker(i)).start()\n",
    "    \n",
    "# Why is this synchronous???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation in main thread. \n",
    "\n",
    "# Use lambda to create a closure...\n",
    "\n",
    "for i in range(4):\n",
    "    threading.Thread(target=lambda: hello_worker(i)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Or use the built-in threading parameter args:\n",
    "\n",
    "for i in range(4):\n",
    "    threading.Thread(target=hello_worker, args=(i,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_timeit(f):\n",
    "    \n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "        print('func:%r args:[%r, %r] took: %2.4f sec' % (f.__name__, args, kw, te-ts))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@my_timeit\n",
    "def bad_fib(n):\n",
    "\n",
    "    def recurse(n):\n",
    "        if n == 0 or n == 1:\n",
    "            return n\n",
    "        else:\n",
    "            return recurse(n-1) + recurse(n-2)\n",
    "    print(recurse(n))\n",
    "\n",
    "threading.Thread(target=bad_fib, args=(35,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Started thread %r' %(i))\n",
    "    threading.Thread(target=bad_fib, args=(35,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting thread-level scheduler thrashing. Behavior used to be A LOT worse ten years ago!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python & Kernel Threads\n",
    "\n",
    "Python threads are not green. 1 Python thread = 1 OS thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threading.Thread API\n",
    "\n",
    "The threading interface in Python is similar to that of Java in that there are two main ways to write threads:\n",
    "1. The functional (and pythonic) way seen above\n",
    "2. By subclassing Thread, e.g.:\n",
    "\n",
    "```\n",
    "class MyThread(threading.Thread):\n",
    "\n",
    "    def run(self):\n",
    "        logging.debug('running')\n",
    "        return\n",
    "\n",
    "for i in range(5):\n",
    "    t = MyThread()\n",
    "    t.start()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C Extensions\n",
    "\n",
    "You can get \"native\" thread parallelism by writing a C extension.\n",
    "The original plan here was to write a parallelized vector math C extension, but while writing this I realized it required too much prior knowledge in Cpython internals, C programming, and some parallel API that would render the exercise not useful given time constraints. However, I will post a full tutorial on my Github later this week on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further materials to study\n",
    "\n",
    "1. David Beazley - [Understanding the Python GIL](https://www.youtube.com/watch?v=Obt-vMVdM8s)\n",
    "2. Philip Guo's fun [CPython internals lecture series](http://www.pgbovine.net/cpython-internals.htm)\n",
    "3. Larry Hastings - [Gilectomy](https://www.youtube.com/watch?v=P3AyI_u66Bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write an echo server using `threading.Thread`\n",
    "2. An intimidating but doable project (I promise!): Referencing just [RFC 1459](https://tools.ietf.org/html/rfc1459), implement an IRC server and IRC client using `threading.Thread`.\n",
    "3. Extend the dummy C extension module to include a full suite of parallelized vector math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locks\n",
    "\n",
    "Let's step away from Python for a bit to review some basic synchronization concepts.\n",
    "\n",
    "The GIL is great in that we users get certain guarantees about the absence of deadlocks, race conditions in the garbage collector, etc. But if we're going to write our own threaded code, we might need to get into the business of locks...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic race condition\n",
    "        \n",
    "class JobQueue:\n",
    "    def __init__(self):\n",
    "        self.nq = 0\n",
    "        self.q = []\n",
    "\n",
    "    def enqueue(self, job):\n",
    "        self.nq += 1\n",
    "        self.q.append(job)\n",
    "\n",
    "    def dequeue(self):\n",
    "        if self.q:\n",
    "            self.nq -= 1\n",
    "            return self.q.pop(-1)\n",
    "\n",
    "class PrintJob:\n",
    "    def __init__(self, s):\n",
    "        self.s = s\n",
    "\n",
    "    def work(self):\n",
    "        #print(self.s)\n",
    "        return self.s\n",
    "\n",
    "j = JobQueue()\n",
    "\n",
    "def producer():\n",
    "    jobs = [PrintJob(str(i)) for i in range(10)]\n",
    "    for i in range(100000):\n",
    "        j.enqueue(jobs[i % 10])\n",
    "\n",
    "def consumer():\n",
    "    while j.q:\n",
    "        j.dequeue().work()\n",
    "        \n",
    "\n",
    "t1 = threading.Thread(target=producer)\n",
    "t1.start()\n",
    "t2 = threading.Thread(target=producer)\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "t3 = threading.Thread(target=consumer)\n",
    "t3.start()\n",
    "t3.join()\n",
    "\n",
    "print(len(j.q))\n",
    "print(j.nq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed!\n",
    "\n",
    "class JobQueue:\n",
    "    def __init__(self):\n",
    "        self.nq = 0\n",
    "        self.q = []\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def enqueue(self, job):\n",
    "        with self.lock:\n",
    "            self.nq += 1\n",
    "            self.q.append(job)\n",
    "\n",
    "    def dequeue(self):\n",
    "        with self.lock:\n",
    "            if self.q:\n",
    "                self.nq -= 1\n",
    "                return self.q.pop(-1)\n",
    "\n",
    "class PrintJob:\n",
    "    def __init__(self, s):\n",
    "        self.s = s\n",
    "\n",
    "    def work(self):\n",
    "        #print(self.s)\n",
    "        return self.s\n",
    "\n",
    "j = JobQueue()\n",
    "\n",
    "def producer():\n",
    "    jobs = [PrintJob(str(i)) for i in range(10)]\n",
    "    for i in range(100000):\n",
    "        j.enqueue(jobs[i % 10])\n",
    "\n",
    "def consumer():\n",
    "    while j.q:\n",
    "        j.dequeue().work()\n",
    "        \n",
    "\n",
    "t1 = threading.Thread(target=producer)\n",
    "t1.start()\n",
    "t2 = threading.Thread(target=producer)\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "t3 = threading.Thread(target=consumer)\n",
    "t3.start()\n",
    "t3.join()\n",
    "\n",
    "print(len(j.q))\n",
    "print(j.nq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadlock\n",
    "\n",
    "Given locks L1 and L2, two threads T1 and T2.  \n",
    "\n",
    "T1 tries to acquire L1 then L2. \n",
    "T2 tries to acquire L2 then L1.\n",
    "\n",
    "1. T1 acquires L1.\n",
    "2. T2 acquires L2.\n",
    "3. T1 tries to acquire L2. Can't. Block.\n",
    "4. T2 tries to acquire L1. Can't. Block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffman Conditions\n",
    "\n",
    "Deadlocks can occur in a system iff:\n",
    "1. Mutual exclusion: At least one resource must be held in a non-shareable mode. Otherwise, the processes would not be prevented from using the resource when necessary. Only one process can use the resource at any given instant of time.\n",
    "2. Hold and wait or resource holding: a process is currently holding at least one resource and requesting additional resources which are being held by other processes.\n",
    "3. No preemption: a resource can be released only voluntarily by the process holding it.\n",
    "4. Circular wait: each process must be waiting for a resource which is being held by another process, which in turn is waiting for the first process to release the resource. In general, there is a set of waiting processes, P = {P1, P2, …, PN}, such that P1 is waiting for a resource held by P2, P2 is waiting for a resource held by P3 and so on until PN is waiting for a resource held by P1.\n",
    "\n",
    "This suggests four distinct methods to guarantee deadlock-free systems:\n",
    "\n",
    "1. No Mutual Exclusion: Have no non-sharable data. This can be achieved by reducing reliance on non-thread-local state, or by relying on non-mutable, read-only data (this will be discussed at length later)\n",
    "2. No Hold and Wait: Prevent threads from requesting new locks if they already have one, or give up a lock if they need a new one (resource intensive)\n",
    "3. Preemption: If a thread is hogging a resource, save its state and unlock the resource. Restore the state later on. \n",
    "4. No Circular Wait: One interesting technique here is to impose an ordering on all locks (L1, L2, ...) and modify code such that no thread can wait on a lock that has a lower ordinal position than all the locks they currently possess. One common ordering heuristic is the memory address of the mutex. Obviously, it is difficult to optimize performance while relying on such an ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> And the problem with almost every published example I see on how to use locks is that they are way too simple. You've got one little resource, one lock to acquire, one to release... and because they show you a simple example it creates the illusion that locks are easy to use. But when you start to put them in larger systems outside of Operating Systems [college classes] you'll find that if you add enough locks it becomes insanely difficult to reason about your code.\n",
    "\n",
    "Raymond Hettinger - [PyBay 2017 Keynote on Concurrency](https://www.youtube.com/watch?v=9zinZmE3Ogk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Material\n",
    "\n",
    "1. Read the Little Book of Semaphores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Implement a solution to the [Dining Philosophers Problem](https://en.wikipedia.org/wiki/Dining_philosophers_problem) using just Locks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queues\n",
    "\n",
    "\"Do not communicate by sharing memory; instead, share memory by communicating.\"\n",
    "\n",
    "Thread-safe queues are one primitive that allow us to avoid mutual exclusion. Instead of locking down shared mutable state, threads/processes can communicate state via queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instead of having the producer and consumer workers interact directly\n",
    "with your state, we have a thread-safe message queue mediating.\n",
    "'''\n",
    "from queue import Queue\n",
    "\n",
    "#This is the same as before\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "\n",
    "    def increment(self):\n",
    "        self.count += 1\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "# Message Queue is now the interface to the Counter\n",
    "message_queue = Queue()\n",
    "def message_queue_worker():\n",
    "    while True:\n",
    "        command = message_queue.get()\n",
    "        if command == 'increment':\n",
    "            c.increment()\n",
    "\n",
    "\n",
    "def producer():\n",
    "    for i in range(100000):\n",
    "        message_queue.put('increment')\n",
    "\n",
    "        \n",
    "mqw = threading.Thread(target=message_queue_worker)\n",
    "mqw.start()\n",
    "\n",
    "t1 = threading.Thread(target=producer)\n",
    "t1.start()\n",
    "t2 = threading.Thread(target=producer)\n",
    "t2.start()\n",
    "\n",
    "while True:\n",
    "    print('Counter: %r' % c.count)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple concept which doesn't require much more commentary, but just want to emphasize how useful this paradigm is. This concept of \"sharing memory by communicating\" is not just useful for writing safe and easy-to-understand concurrent servers/clients, but is useful on the distributed system level. Queueing services like AWS SQS and RabbitMQ essentially are means to achieve the same design goals, but on a larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delicious parallelism. Obvious gains for CPU bound tasks. Python processes are however quite heavy. Not to be used for, say, handling a large number of client connections concurrently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the working Python programmer: `multiprocessing` tends to be a bit more robust and feature complete than `threading`. The interface for `multiprocessing` is very similar to `threading` however. \n",
    "\n",
    "Processes are often interacted with via Pools. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def bad_fib2(n):\n",
    "    def recurse(n):\n",
    "        if n == 0 or n == 1:\n",
    "            return n\n",
    "        else:\n",
    "            return recurse(n-1) + recurse(n-2)\n",
    "    print(recurse(n))\n",
    "    \n",
    "p = multiprocessing.Pool(1)\n",
    "\n",
    "@my_timeit\n",
    "def foo():\n",
    "    p.map(bad_fib2, [35, 35, 35])\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = multiprocessing.Pool(3)\n",
    "\n",
    "@my_timeit\n",
    "def foo():\n",
    "    p.map(bad_fib2, [35, 35, 35])\n",
    "\n",
    "foo()\n",
    "#Check htop..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For synchronization, `multiprocessing` contains `Lock`s and shared memory objects like `Value` and `Array`. I would maintain though that interprocess state is best implemented via `multiprocessing.Queue` (and similarly via `multiprocessing.Pipe` which produces a pair of reading/writing sockets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Problem: Count the number of appearances of words in a set of documents,\n",
    "leveraging a multiprocessing pipeline\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "urls = ['https://en.wikipedia.org/wiki/MapReduce',\n",
    "        'https://en.wikipedia.org/wiki/Computer_cluster',\n",
    "        'https://en.wikipedia.org/wiki/Operating_system',\n",
    "        'https://en.wikipedia.org/wiki/Linux',\n",
    "        'https://en.wikipedia.org/wiki/GNU_General_Public_License',\n",
    "        'https://en.wikipedia.org/wiki/End_user',\n",
    "        'https://en.wikipedia.org/wiki/Light-emitting_diode',\n",
    "        'https://en.wikipedia.org/wiki/James_R._Biard'\n",
    "       ]\n",
    "\n",
    "def scrape_one(url):\n",
    "    words = Counter()\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text)\n",
    "    ps = soup.find_all('p')\n",
    "    for p in ps:\n",
    "        words.update(p.text.split(' '))\n",
    "    return words\n",
    "\n",
    "@my_timeit\n",
    "def scrape_all_and_merge(urls):\n",
    "    words = Counter()\n",
    "    for url in urls:\n",
    "        words.update(scrape_one(url))\n",
    "    return words\n",
    "\n",
    "scrape_all_and_merge(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def scrape_all_and_merge(urls):\n",
    "    words = Counter()\n",
    "    for url in urls:\n",
    "        words.update(scrape_one(url))\n",
    "    return words\n",
    "\n",
    "n = 4\n",
    "pool = multiprocessing.Pool(n)\n",
    "\n",
    "def split_chunk(l, n):\n",
    "    #Split l into a list of n lists\n",
    "    step = len(l) // n\n",
    "    return [l[i*step:(i+1)*step:] for i in range(n)]\n",
    "\n",
    "split_urls = split_chunk(urls, n)\n",
    "\n",
    "@my_timeit\n",
    "def foo():\n",
    "    words = Counter()\n",
    "    counter_list = pool.map(scrape_all_and_merge, split_urls)\n",
    "    for c in counter_list:\n",
    "        words.update(c)\n",
    "    print(words)\n",
    "\n",
    "foo()\n",
    "\n",
    "#Generalize into a recursive structure of processing and reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
